{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]]))"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "a = np.zeros(25)\n",
    "b = a.reshape(5,5)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 0., 0., 0., 0.],\n       [0., 1., 2., 0., 0.],\n       [0., 2., 1., 0., 0.],\n       [0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "b[2,2]=1\n",
    "b[1,1]=1\n",
    "b[3,3]=1\n",
    "b[2,1]=2\n",
    "b[1,2]=2\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 2.],\n        [0., 0., 0., 0., 0.]]])"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "c = np.zeros(3*5*5)\n",
    "c = c.reshape(3,5,5)\n",
    "c[2,3,4] = 2\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array([2], dtype=int64), array([3], dtype=int64), array([4], dtype=int64))"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "y = np.where(c == 2)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[[ 0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.]],\n\n       [[ 0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.]],\n\n       [[ 0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0., 10.],\n        [ 0.,  0.,  0.,  0.,  0.]]])"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "c[y] = 10\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.4143, 0.7250, 0.9155, 0.3366],\n        [0.6844, 0.7806, 0.8983, 0.0224],\n        [0.5035, 0.6321, 0.5026, 0.7428],\n        [0.5304, 0.9010, 0.6342, 0.4330]])\ntensor([[0.4143, 0.7250, 0.9155, 0.3366],\n        [0.6844, 0.7806, 0.8983, 0.0224],\n        [0.5035, 0.6321, 0.5026, 0.7428]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(4,4)\n",
    "print(a)\n",
    "print(a[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_augment.py\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "# from PIL import Image\n",
    "\n",
    "\n",
    "# 椒盐噪声（输入图像，噪声个数）\n",
    "def salt(img, n):\n",
    "    for k in range(n):\n",
    "        i = int(np.random.random() * img.shape[1])\n",
    "        j = int(np.random.random() * img.shape[0])\n",
    "        if img.ndim == 2:\n",
    "            img[j, i] = 255\n",
    "        elif img.ndim == 3:\n",
    "            img[j, i, 0] = 255\n",
    "            img[j, i, 1] = 255\n",
    "            img[j, i, 2] = 255\n",
    "    return img\n",
    "\n",
    "# 椒盐噪声\n",
    "def SaltAndPepper(src,percetage):\n",
    "    SP_NoiseImg=src.copy()\n",
    "    SP_NoiseNum=int(percetage*src.shape[0]*src.shape[1])\n",
    "    for i in range(SP_NoiseNum):\n",
    "        randR=np.random.randint(0,src.shape[0]-1)\n",
    "        randG=np.random.randint(0,src.shape[1]-1)\n",
    "        randB=np.random.randint(0,3)\n",
    "        if np.random.randint(0,1)==0:\n",
    "            SP_NoiseImg[randR,randG,randB]=0\n",
    "        else:\n",
    "            SP_NoiseImg[randR,randG,randB]=255\n",
    "    return SP_NoiseImg\n",
    "\n",
    "# 高斯噪声\n",
    "def addGaussianNoise(image,percetage):\n",
    "    G_Noiseimg = image.copy()\n",
    "    w = image.shape[1]\n",
    "    h = image.shape[0]\n",
    "    G_NoiseNum=int(percetage*image.shape[0]*image.shape[1])\n",
    "    for i in range(G_NoiseNum):\n",
    "        temp_x = np.random.randint(0,h)\n",
    "        temp_y = np.random.randint(0,w)\n",
    "        G_Noiseimg[temp_x][temp_y][np.random.randint(3)] = np.random.randn(1)[0]\n",
    "    return G_Noiseimg\n",
    "\n",
    "# 昏暗\n",
    "def darker(image,percetage=0.8):\n",
    "    image_copy = image.copy()\n",
    "    w = image.shape[1]\n",
    "    h = image.shape[0]\n",
    "    #get darker\n",
    "    for xi in range(0,w):\n",
    "        for xj in range(0,h):\n",
    "            image_copy[xj,xi,0] = int(image[xj,xi,0]*percetage)\n",
    "            image_copy[xj,xi,1] = int(image[xj,xi,1]*percetage)\n",
    "            image_copy[xj,xi,2] = int(image[xj,xi,2]*percetage)\n",
    "    return image_copy\n",
    "\n",
    "# 亮度\n",
    "def brighter(image, percetage=1.4):\n",
    "    image_copy = image.copy()\n",
    "    w = image.shape[1]\n",
    "    h = image.shape[0]\n",
    "    #get brighter\n",
    "    for xi in range(0,w):\n",
    "        for xj in range(0,h):\n",
    "            image_copy[xj,xi,0] = np.clip(int(image[xj,xi,0]*percetage),a_max=255,a_min=0)\n",
    "            image_copy[xj,xi,1] = np.clip(int(image[xj,xi,1]*percetage),a_max=255,a_min=0)\n",
    "            image_copy[xj,xi,2] = np.clip(int(image[xj,xi,2]*percetage),a_max=255,a_min=0)\n",
    "    return image_copy\n",
    "\n",
    "# 旋转\n",
    "def rotate(image, angle, center=None, scale=1.0):\n",
    "    (h, w) = image.shape[:2]\n",
    "    # If no rotation center is specified, the center of the image is set as the rotation center\n",
    "    if center is None:\n",
    "        center = (w / 2, h / 2)\n",
    "    m = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "    rotated = cv2.warpAffine(image, m, (w, h))\n",
    "    return rotated\n",
    "\n",
    "# 翻转\n",
    "def flip(image):\n",
    "    flipped_image = np.fliplr(image)\n",
    "    return flipped_image\n",
    "\n",
    "class AUGMENT_data():\n",
    "    def __init__(self, data_path):\n",
    "        # 初始化函数，读取所有data_path下的图片\n",
    "        self.data_path = data_path\n",
    "        self.imgs_paths = glob.glob(os.path.join(data_path, 'imgs/*.jpg'))\n",
    "        self.image_bright = 0\n",
    "        self.image_path = ''\n",
    "        self.label_path = ''\n",
    "    \n",
    "    def augment_img(self, img, lab):\n",
    "        image = img\n",
    "        label = lab\n",
    "        # print(self.image_bright)\n",
    "        #变亮、变暗\n",
    "        if self.image_bright > 180:\n",
    "            img_b = darker(image)\n",
    "            save_imgres_path = self.image_path.split('.')[0] + '_darker.jpg'\n",
    "            save_labelres_path = self.label_path.split('.')[0] + '_darker.png'\n",
    "            cv2.imwrite(save_imgres_path, img_b)\n",
    "            cv2.imwrite(save_labelres_path, label)\n",
    "        elif self.image_bright < 100:\n",
    "            img_b = brighter(image)\n",
    "            save_imgres_path = self.image_path.split('.')[0] + '_bright.jpg'\n",
    "            save_labelres_path = self.label_path.split('.')[0] + '_bright.png'\n",
    "            cv2.imwrite(save_imgres_path, img_b)\n",
    "            cv2.imwrite(save_labelres_path, label)\n",
    "\n",
    "        # 旋转\n",
    "        rotated_90 = rotate(image, 90)\n",
    "        rotated_90_label = rotate(label, 90)\n",
    "        save_imgres_path = self.image_path.split('.')[0] + '_rotated_90.jpg'\n",
    "        save_labelres_path = self.label_path.split('.')[0] + '_rotated_90.png'\n",
    "        cv2.imwrite(save_imgres_path, rotated_90)\n",
    "        cv2.imwrite(save_labelres_path, rotated_90_label)\n",
    "\n",
    "        rotated_180 = rotate(image, 180)\n",
    "        rotated_180_label = rotate(label, 180)\n",
    "        save_imgres_path = self.image_path.split('.')[0] + '_rotated_180.jpg'\n",
    "        save_labelres_path = self.label_path.split('.')[0] + '_rotated_180.png'\n",
    "        cv2.imwrite(save_imgres_path, rotated_180)\n",
    "        cv2.imwrite(save_labelres_path, rotated_180_label)\n",
    "\n",
    "        # 镜像\n",
    "        flipCode = random.choice([-1, 0, 1])\n",
    "        flipped_img = cv2.flip(image, flipCode)\n",
    "        flipped_label = cv2.flip(label, flipCode)\n",
    "        save_imgres_path = self.image_path.split('.')[0] + '_flipped.jpg'\n",
    "        save_labelres_path = self.label_path.split('.')[0] + '_flipped.png'\n",
    "        cv2.imwrite(save_imgres_path, flipped_img)\n",
    "        cv2.imwrite(save_labelres_path, flipped_label)\n",
    "\n",
    "        # 增加噪声\n",
    "        img_salt = SaltAndPepper(img, 0.2)\n",
    "        save_imgres_path = self.image_path.split('.')[0] + '_salt.jpg'\n",
    "        save_labelres_path = self.label_path.split('.')[0] + '_salt.png'\n",
    "        cv2.imwrite(save_imgres_path, img_salt)\n",
    "        cv2.imwrite(save_labelres_path, label)\n",
    "\n",
    "        img_gauss = addGaussianNoise(img, 0.2)\n",
    "        save_imgres_path = self.image_path.split('.')[0] + '_gauss.jpg'\n",
    "        save_labelres_path = self.label_path.split('.')[0] + '_gauss.png'\n",
    "        cv2.imwrite(save_imgres_path, img_gauss)\n",
    "        cv2.imwrite(save_labelres_path, label)\n",
    "\n",
    "    def getitem(self):\n",
    "        for image_path in self.imgs_paths:\n",
    "            # 根据image_path生成label_path\n",
    "            label_path = image_path.replace('imgs', 'masks')\n",
    "            label_path = label_path.replace('jpg', 'png')\n",
    "            self.image_path = image_path\n",
    "            self.label_path = label_path\n",
    "            # 读取训练图片和标签图片\n",
    "            image = cv2.imread(image_path)\n",
    "            label = cv2.imread(label_path)\n",
    "            self.image_bright = image.mean()\n",
    "            print(image.shape)\n",
    "            # print(image.mean())\n",
    "            # print(type(image))\n",
    "            # print(image.max())\n",
    "            # print(image.min())\n",
    "            print(label.shape)\n",
    "            # 进行数据增广，并保存\n",
    "            image = self.augment_img(image, label)\n",
    "            print('saved ********')\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回训练集大小\n",
    "        return len(self.imgs_paths)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    augment_data = AUGMENT_data(\"data_augment/\")\n",
    "    print(\"数据个数：\", len(augment_data))\n",
    "    augment_data.getitem()\n",
    "    print('end')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图像的亮度、对比度、色度和锐度四种方式的增强（或减弱）处理\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from PIL import ImageEnhance\n",
    "\n",
    "# 原始图像\n",
    "def ImageAugument():\n",
    "    path = r'C:\\\\Users\\\\hkc\\\\Desktop\\\\meter_img\\\\temporary_test'  # 文件夹目录\n",
    "    files = os.listdir(path)  # 得到文件夹下的所有文件名称\n",
    "    # 遍历文件夹\n",
    "    prefix = path + '/'\n",
    "    for file in files:\n",
    "        # print(file)\n",
    "        image = Image.open(prefix + file)\n",
    "        # image.show()\n",
    "\n",
    "        # 亮度增强\n",
    "        enh_bri = ImageEnhance.Brightness(image)\n",
    "        brightness = 1.5\n",
    "        image_brightened = enh_bri.enhance(brightness)\n",
    "        image_brightened.save(prefix + file.strip('.jpg') + '-lightup' + '.jpg')\n",
    "        # 亮度减弱\n",
    "        enh_bri = ImageEnhance.Brightness(image)\n",
    "        brightness = 0.5\n",
    "        image_brightened = enh_bri.enhance(brightness)\n",
    "        image_brightened.save(prefix + file.strip('.jpg') + '-lightdown' + '.jpg')\n",
    "\n",
    "        # 色度增强\n",
    "        enh_col = ImageEnhance.Color(image)\n",
    "        color = 1.5\n",
    "        image_colored = enh_col.enhance(color)\n",
    "        image_colored.save(prefix + file.strip('.jpg') + '-colorup' + '.jpg')\n",
    "        # 色度减弱\n",
    "        enh_col = ImageEnhance.Color(image)\n",
    "        color = 0.5\n",
    "        image_colored = enh_col.enhance(color)\n",
    "        image_colored.save(prefix + file.strip('.jpg') + '-colordown' + '.jpg')\n",
    "\n",
    "        # 对比度增强\n",
    "        enh_con = ImageEnhance.Contrast(image)\n",
    "        contrast = 1.7\n",
    "        image_contrasted = enh_con.enhance(contrast)\n",
    "        image_contrasted.save(prefix + file.strip('.jpg') + '-contrastup' + '.jpg')\n",
    "        # 对比度减弱\n",
    "        enh_con = ImageEnhance.Contrast(image)\n",
    "        contrast = 0.7\n",
    "        image_contrasted = enh_con.enhance(contrast)\n",
    "        image_contrasted.save(prefix + file.strip('.jpg') + '-contrastdown' + '.jpg')\n",
    "\n",
    "        # # 锐度增强\n",
    "        # enh_sha = ImageEnhance.Sharpness(image)\n",
    "        # sharpness = 3.0\n",
    "        # image_sharped = enh_sha.enhance(sharpness)\n",
    "        # image_sharped.save(prefix + file.strip('.jpg') + '-moreSharup' + '.jpg')\n",
    "        # # 锐度减弱\n",
    "        # enh_sha = ImageEnhance.Sharpness(image)\n",
    "        # sharpness = 1.5\n",
    "        # image_sharped = enh_sha.enhance(sharpness)\n",
    "        # image_sharped.save(prefix + file.strip('.jpg') + '-moreShardown' + '.jpg')\n",
    "\n",
    "        # 椒盐噪声\n",
    "        SP_NoiseImg = image.copy()\n",
    "        pim = SP_NoiseImg.load()\n",
    "        percetage = 0.1\n",
    "        SP_NoiseNum=int(percetage*image.width*image.height)\n",
    "        for i in range(SP_NoiseNum):\n",
    "            randR=np.random.randint(0,image.width-1)\n",
    "            randG=np.random.randint(0,image.height-1)\n",
    "            randB=np.random.randint(0,3)\n",
    "            pim[randR,randG]=(0,0,0)\n",
    "        SP_NoiseImg.save(prefix + file.strip('.jpg') + '-Noise' + '.jpg')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ImageAugument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.81it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    " \n",
    "for i in tqdm(range(100)):\n",
    "  time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "img1 =  tensor([[[0.6078, 0.6196, 0.6157,  ..., 0.6314, 0.4667, 0.4431],\n",
      "         [0.6314, 0.6078, 0.6039,  ..., 0.6157, 0.5059, 0.4588],\n",
      "         [0.6431, 0.6196, 0.6118,  ..., 0.6118, 0.5176, 0.4863],\n",
      "         ...,\n",
      "         [0.7255, 0.7294, 0.7294,  ..., 0.7333, 0.7333, 0.7333],\n",
      "         [0.7255, 0.7294, 0.7294,  ..., 0.7333, 0.7333, 0.7333],\n",
      "         [0.7255, 0.7294, 0.7294,  ..., 0.7333, 0.7333, 0.7333]],\n",
      "\n",
      "        [[0.6275, 0.6392, 0.6353,  ..., 0.6353, 0.4706, 0.4471],\n",
      "         [0.6510, 0.6275, 0.6235,  ..., 0.6196, 0.5098, 0.4627],\n",
      "         [0.6627, 0.6392, 0.6314,  ..., 0.6157, 0.5216, 0.4902],\n",
      "         ...,\n",
      "         [0.7255, 0.7294, 0.7294,  ..., 0.7333, 0.7333, 0.7333],\n",
      "         [0.7255, 0.7294, 0.7294,  ..., 0.7333, 0.7333, 0.7333],\n",
      "         [0.7255, 0.7294, 0.7294,  ..., 0.7333, 0.7333, 0.7333]],\n",
      "\n",
      "        [[0.6039, 0.6157, 0.6118,  ..., 0.6118, 0.4471, 0.4235],\n",
      "         [0.6275, 0.6039, 0.6000,  ..., 0.5961, 0.4863, 0.4392],\n",
      "         [0.6392, 0.6157, 0.6078,  ..., 0.5922, 0.4980, 0.4667],\n",
      "         ...,\n",
      "         [0.7255, 0.7294, 0.7294,  ..., 0.7333, 0.7333, 0.7333],\n",
      "         [0.7255, 0.7294, 0.7294,  ..., 0.7333, 0.7333, 0.7333],\n",
      "         [0.7255, 0.7294, 0.7294,  ..., 0.7333, 0.7333, 0.7333]]])\n",
      "img2 =  tensor([[[0.6039, 0.6157, 0.6118,  ..., 0.6118, 0.4471, 0.4235],\n",
      "         [0.6275, 0.6039, 0.6000,  ..., 0.5961, 0.4863, 0.4392],\n",
      "         [0.6392, 0.6157, 0.6078,  ..., 0.5922, 0.4980, 0.4667],\n",
      "         ...,\n",
      "         [0.7255, 0.7294, 0.7294,  ..., 0.7333, 0.7333, 0.7333],\n",
      "         [0.7255, 0.7294, 0.7294,  ..., 0.7333, 0.7333, 0.7333],\n",
      "         [0.7255, 0.7294, 0.7294,  ..., 0.7333, 0.7333, 0.7333]],\n",
      "\n",
      "        [[0.6275, 0.6392, 0.6353,  ..., 0.6353, 0.4706, 0.4471],\n",
      "         [0.6510, 0.6275, 0.6235,  ..., 0.6196, 0.5098, 0.4627],\n",
      "         [0.6627, 0.6392, 0.6314,  ..., 0.6157, 0.5216, 0.4902],\n",
      "         ...,\n",
      "         [0.7255, 0.7294, 0.7294,  ..., 0.7333, 0.7333, 0.7333],\n",
      "         [0.7255, 0.7294, 0.7294,  ..., 0.7333, 0.7333, 0.7333],\n",
      "         [0.7255, 0.7294, 0.7294,  ..., 0.7333, 0.7333, 0.7333]],\n",
      "\n",
      "        [[0.6078, 0.6196, 0.6157,  ..., 0.6314, 0.4667, 0.4431],\n",
      "         [0.6314, 0.6078, 0.6039,  ..., 0.6157, 0.5059, 0.4588],\n",
      "         [0.6431, 0.6196, 0.6118,  ..., 0.6118, 0.5176, 0.4863],\n",
      "         ...,\n",
      "         [0.7255, 0.7294, 0.7294,  ..., 0.7333, 0.7333, 0.7333],\n",
      "         [0.7255, 0.7294, 0.7294,  ..., 0.7333, 0.7333, 0.7333],\n",
      "         [0.7255, 0.7294, 0.7294,  ..., 0.7333, 0.7333, 0.7333]]])\n",
      "img_2 =  <PIL.Image.Image image mode=RGB size=400x397 at 0x24D232F50B8>\n"
     ]
    }
   ],
   "source": [
    "# https://zhuanlan.zhihu.com/p/27382990\n",
    "# PIL.Image/numpy.ndarray与Tensor的相互转换 \n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "img_path = \"E:\\\\sc\\\\image_data\\\\meter\\\\meter_seg\\\\images\\\\test_true\\\\1.jpg\"\n",
    "\n",
    "# transforms.ToTensor()\n",
    "transform1 = transforms.Compose([\n",
    "    transforms.ToTensor(), # range [0, 255] -> [0.0,1.0]\n",
    "    ]\n",
    ")\n",
    "\n",
    "##numpy.ndarray\n",
    "img = cv2.imread(img_path)# 读取图像\n",
    "# cv2.imshow('img', img)\n",
    "img1 = transform1(img) # 归一化到 [0.0,1.0]\n",
    "print(\"img1 = \",img1)\n",
    "# 转化为numpy.ndarray并显示\n",
    "img_1 = img1.numpy()*255\n",
    "img_1 = img_1.astype('uint8')\n",
    "img_1 = np.transpose(img_1, (1,2,0))\n",
    "cv2.imshow('img_1', img_1)\n",
    "cv2.waitKey()\n",
    "\n",
    "##PIL\n",
    "img = Image.open(img_path).convert('RGB') # 读取图像\n",
    "img2 = transform1(img) # 归一化到 [0.0,1.0]\n",
    "print(\"img2 = \",img2)\n",
    "#转化为PILImage并显示\n",
    "img_2 = transforms.ToPILImage()(img2).convert('RGB')\n",
    "print(\"img_2 = \",img_2)\n",
    "img_2.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 归一化\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = (0.5, 0.5, 0.5), std = (0.5, 0.5, 0.5))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIL.Image的缩放裁剪等操作\n",
    "# transforms.RandomCrop()\n",
    "transform4 = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomCrop((300,300)),\n",
    "    transforms.Resize((256, 256)), \n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img3 = transform4(img)\n",
    "img3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "img1 =  tensor([[[0.6275, 0.5922, 0.5804,  ..., 0.7647, 0.7647, 0.7725],\n         [0.6314, 0.6118, 0.5961,  ..., 0.7686, 0.7608, 0.7608],\n         [0.6353, 0.6235, 0.6039,  ..., 0.7569, 0.7569, 0.7529],\n         ...,\n         [0.7451, 0.7451, 0.7451,  ..., 0.7608, 0.7451, 0.7490],\n         [0.7451, 0.7451, 0.7451,  ..., 0.7490, 0.7529, 0.7529],\n         [0.7451, 0.7451, 0.7451,  ..., 0.7451, 0.7412, 0.7373]],\n\n        [[0.6549, 0.6314, 0.6275,  ..., 0.7647, 0.7647, 0.7725],\n         [0.6627, 0.6510, 0.6431,  ..., 0.7725, 0.7647, 0.7725],\n         [0.6627, 0.6588, 0.6510,  ..., 0.7725, 0.7686, 0.7725],\n         ...,\n         [0.7451, 0.7451, 0.7451,  ..., 0.7373, 0.7216, 0.7255],\n         [0.7451, 0.7451, 0.7451,  ..., 0.7294, 0.7294, 0.7333],\n         [0.7451, 0.7451, 0.7451,  ..., 0.7255, 0.7176, 0.7176]],\n\n        [[0.6510, 0.6275, 0.6235,  ..., 0.7647, 0.7647, 0.7725],\n         [0.6588, 0.6471, 0.6353,  ..., 0.7686, 0.7647, 0.7647],\n         [0.6588, 0.6549, 0.6431,  ..., 0.7608, 0.7608, 0.7569],\n         ...,\n         [0.7451, 0.7451, 0.7451,  ..., 0.7569, 0.7412, 0.7451],\n         [0.7451, 0.7451, 0.7451,  ..., 0.7490, 0.7490, 0.7451],\n         [0.7451, 0.7451, 0.7451,  ..., 0.7451, 0.7373, 0.7294]]])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# https://www.aiuai.cn/aifarm1380.html\n",
    "# 2.基于 albumentations 的 pipline, opencv 版\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from albumentations import Compose, RandomCrop, Normalize, HorizontalFlip, Resize, ShiftScaleRotate\n",
    "from albumentations.pytorch import ToTensor\n",
    "\n",
    "img_path = \"E:\\\\sc\\\\image_data\\\\meter\\\\meter_seg\\\\images\\\\test_true\\\\1.jpg\"\n",
    "\n",
    "albumentations_transform = Compose([\n",
    "    Resize(256, 256), \n",
    "    RandomCrop(224, 224),\n",
    "    HorizontalFlip(),\n",
    "    ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=45, p=.75),\n",
    "    # Normalize(\n",
    "    #     mean=[0.485, 0.456, 0.406],\n",
    "    #     std=[0.229, 0.224, 0.225],\n",
    "    # ),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "img = cv2.imread(img_path)# 读取图像\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "augmented = albumentations_transform(image=img)\n",
    "img4 = augmented['image']\n",
    "print(\"img1 = \",img4)\n",
    "# 转化为numpy.ndarray并显示\n",
    "img_4 = img4.numpy()*255\n",
    "img_4 = img_4.astype('uint8')\n",
    "img_4 = np.transpose(img_4, (1,2,0))\n",
    "cv2.imshow('img_4', img_4)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 包含上一层目录\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://zhuanlan.zhihu.com/p/159379768\n",
    "# pytorch模型转ONNX模型\n",
    "# 1、文件中保存模型结构和权重参数\n",
    "import torch\n",
    "torch_model = torch.load(\"save.pt\") # pytorch模型加载\n",
    "batch_size = 1  #批处理大小\n",
    "input_shape = (3,244,244)   #输入数据\n",
    "\n",
    "# set the model to inference mode\n",
    "torch_model.eval()\n",
    "\n",
    "x = torch.randn(batch_size,*input_shape)\t\t# 生成张量\n",
    "export_onnx_file = \"test.onnx\"\t\t\t\t\t# 目的ONNX文件名\n",
    "torch.onnx.export(torch_model,\n",
    "                    x,\n",
    "                    export_onnx_file,\n",
    "                    opset_version=10,\n",
    "                    do_constant_folding=True,\t# 是否执行常量折叠优化\n",
    "                    input_names=[\"input\"],\t\t# 输入名\n",
    "                    output_names=[\"output\"],\t# 输出名\n",
    "                    dynamic_axes={\"input\":{0:\"batch_size\"},\t\t# 批处理变量\n",
    "                                    \"output\":{0:\"batch_size\"}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 4.00 GiB total capacity; 2.86 GiB already allocated; 486.40 KiB free; 2.89 GiB reserved in total by PyTorch)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-b5641ec460ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m                     \u001b[0moutput_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"output\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m    \u001b[1;31m# 输出名\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                     dynamic_axes={\"input\":{0:\"batch_size\"},\t# 批处理变量\n\u001b[1;32m---> 31\u001b[1;33m                                     \"output\":{0:\"batch_size\"}})\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\p-t-python3.6\\lib\\site-packages\\torch\\onnx\\__init__.py\u001b[0m in \u001b[0;36mexport\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)\u001b[0m\n\u001b[0;32m    206\u001b[0m                         \u001b[0mdo_constant_folding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexample_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m                         \u001b[0mstrip_doc_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdynamic_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_initializers_as_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m                         custom_opsets, enable_onnx_checker, use_external_data_format)\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\p-t-python3.6\\lib\\site-packages\\torch\\onnx\\utils.py\u001b[0m in \u001b[0;36mexport\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mdynamic_axes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdynamic_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_initializers_as_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep_initializers_as_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[0mcustom_opsets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_opsets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menable_onnx_checker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menable_onnx_checker\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             use_external_data_format=use_external_data_format)\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\p-t-python3.6\\lib\\site-packages\\torch\\onnx\\utils.py\u001b[0m in \u001b[0;36m_export\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, propagate, opset_version, _retain_param_name, do_constant_folding, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, enable_onnx_checker, use_external_data_format)\u001b[0m\n\u001b[0;32m    528\u001b[0m                                                             \u001b[0mexample_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpropagate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                                                             \u001b[0m_retain_param_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_do_constant_folding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m                                                             fixed_batch_size=fixed_batch_size)\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m             \u001b[1;31m# TODO: Don't allocate a in-memory string for the protobuf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\p-t-python3.6\\lib\\site-packages\\torch\\onnx\\utils.py\u001b[0m in \u001b[0;36m_model_to_graph\u001b[1;34m(model, args, verbose, input_names, output_names, operator_export_type, example_outputs, propagate, _retain_param_name, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size)\u001b[0m\n\u001b[0;32m    364\u001b[0m             graph, tuple(in_vars), False, propagate)\n\u001b[0;32m    365\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_trace_and_get_graph_from_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m         \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unique_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\p-t-python3.6\\lib\\site-packages\\torch\\onnx\\utils.py\u001b[0m in \u001b[0;36m_trace_and_get_graph_from_model\u001b[1;34m(model, args)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[0mtrace_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs_states\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_trace_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_force_outplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_return_inputs_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m     \u001b[0mwarn_on_static_input_change\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\p-t-python3.6\\lib\\site-packages\\torch\\jit\\__init__.py\u001b[0m in \u001b[0;36m_get_trace_graph\u001b[1;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mONNXTracedModule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_force_outplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_return_inputs_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\p-t-python3.6\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\p-t-python3.6\\lib\\site-packages\\torch\\jit\\__init__.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    424\u001b[0m             \u001b[0m_create_interpreter_name_lookup_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_force_outplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m         )\n\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\p-t-python3.6\\lib\\site-packages\\torch\\jit\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[0mtrace_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m             \u001b[0mret_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_return_inputs_states\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m                 \u001b[0minputs_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_unflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\p-t-python3.6\\lib\\site-packages\\torch\\jit\\__init__.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[0mtrace_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m             \u001b[0mret_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_return_inputs_states\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m                 \u001b[0minputs_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_unflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 4.00 GiB total capacity; 2.86 GiB already allocated; 486.40 KiB free; 2.89 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# 2.文件中只保留模型权重\n",
    "import torch\n",
    "from modeling.deeplab import *\n",
    "\n",
    "model = DeepLab(num_classes=3,\n",
    "                    backbone='resnet',\n",
    "                    output_stride=16,\n",
    "                    sync_bn=None,\n",
    "                    freeze_bn=False)\n",
    "ckpt = torch.load(\"../run/meter_seg_voc/deeplab-resnet/model_best.pth.tar\", map_location='cpu')\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "model = model.cuda()\n",
    "\n",
    "torch_model = model  \t\t\t\t\t# 由研究员提供python.py文件\n",
    "batch_size = 1 \t\t\t\t\t\t\t\t# 批处理大小\n",
    "input_shape = (3, 513, 513) \t\t\t\t# 输入数据\n",
    "\n",
    "# set the model to inference mode\n",
    "torch_model.eval()\n",
    "\n",
    "x = torch.randn(batch_size,*input_shape).cuda() \t# 生成张量\n",
    "export_onnx_file = \"test.onnx\" \t\t\t\t# 目的ONNX文件名\n",
    "torch.onnx.export(torch_model,\n",
    "                    x,\n",
    "                    export_onnx_file,\n",
    "                    opset_version=11,\n",
    "                    do_constant_folding=True,\t# 是否执行常量折叠优化\n",
    "                    input_names=[\"input\"],\t\t# 输入名\n",
    "                    output_names=[\"output\"],\t# 输出名\n",
    "                    dynamic_axes={\"input\":{0:\"batch_size\"},\t# 批处理变量\n",
    "                                    \"output\":{0:\"batch_size\"}})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==> Passed\n"
     ]
    }
   ],
   "source": [
    "# 检查一下生成的onn\n",
    "import onnx\n",
    "\n",
    "test = onnx.load('test.onnx')\n",
    "onnx.checker.check_model(test)\n",
    "print(\"==> Passed\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "aa66ad8133c8fd1d3c48ce4d2cab6d7ce43d61c766b1e0c5e603cc1a33e28b6a"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}